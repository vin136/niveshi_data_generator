{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Resources to look  into:\n",
    "[some random dude's btc trader project](https://github.com/lefnire/tforce_btc_trader)\n",
    "\n",
    "[Open Ai's guide to becoming RL researcher](https://spinningup.openai.com/en/latest/)\n",
    "\n",
    "[Trading as a game](https://arxiv.org/pdf/1807.02787.pdf)\n",
    "\n",
    "[A gym like Trading environment ](https://github.com/Kismuz/btgym)\n",
    "\n",
    "[A tensorflow library for reinforcement learning](https://github.com/reinforceio/tensorforce)\n",
    "\n",
    "[useful research papers](https://arxiv.org/pdf/1811.07522.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://spinningup.openai.com/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* some thoughts on state-of-art algorithms:\n",
    "    * value-iteration(off-policy) : These don't optimize directly for the expected return but still have good data efficiency.Note: The following don't offer any gaurantees of convergance\n",
    "       * Deep-q learning and corresponding hacks.\n",
    "       * Deep Deterministic policy gradient (DDPG)\n",
    "       * Twin Delayed\n",
    "       * soft-actor critic\n",
    "    * On-policy : \n",
    "       * Vanilla policy gradient\n",
    "       * proximal policy optimization(ppo)\n",
    "       * Trust region policy optimization(trpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A thought on Return: We frequently set up algorithms to optimize the undiscounted return, but use discount factors in estimating value functions.(mathematical conveniance..)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A taxonomy of useful  RL algorithms (having atleast a basic understandig of these is  useful)\n",
    "<img src = 'ataxonomy.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important thoughts :\n",
    "* Model free and Model based methods :\n",
    "    * Though it initially seemed to me a bad idea to consider model based methods for trading...it seems more plausible to have a model of the environment(Note: we are not trying to micro-model the environment but only to an extent needed for prediction)[By model i mean a function which predicts state transitions and rewards.]\n",
    "    * The main upside to having a model is that it allows the agent to plan by thinking ahead, seeing what would happen for a range of possible choices, and explicitly deciding between its options.Consider how a `master trader` who is consistently making profits in the market...he evolves a model of the environment ..say..observing some patterns correlate to a high probability bet..similar process can be modeled by the agent. Agents can then distill the results from planning ahead into a learned policy. A particularly famous example of this approach is AlphaZero. When this works, it can result in a substantial improvement in sample efficiency over methods that don’t have a model.By the way ,this is an incredably difficult task ... and surely a path to fame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most important section of this notebook : How to get more data for training than there is ?\n",
    "\n",
    "`Data Augmentation for Model-Free Methods`. Use a model-free RL algorithm to train a policy or Q-function, but either 1) augment real experiences with fictitious ones in updating the agent, or 2) use only fictitous experience for updating the agent.\n",
    "\n",
    "See (MBVE)[https://arxiv.org/abs/1803.00101] for an example of augmenting real experiences with fictitious ones.\n",
    "See (World Models)[https://worldmodels.github.io/] for an example of using purely fictitious experience to train the agent, which they call “training in the dream.”\n",
    "\n",
    "Embedding Planning Loops into Policies. Another approach embeds the planning procedure directly into a policy as a subroutine—so that complete plans become side information for the policy—while training the output of the policy with any standard model-free algorithm. The key concept is that in this framework, the policy can learn to choose how and when to use the plans. This makes model bias less of a problem, because if the model is bad for planning in some states, the policy can simply learn to ignore it.\n",
    "\n",
    "See (I2A)[https://arxiv.org/abs/1707.06203] for an example of agents being endowed with this style of imagination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An intro into Deep Reinforcement Learning(not intended as a study guide but a walk-through of the most promising algorithms with added gotchas for practical implementation..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.684, reward_mean=24.1, reward_bound=23.0\n",
      "1: loss=0.611, reward_mean=19.9, reward_bound=23.0\n",
      "2: loss=0.530, reward_mean=14.6, reward_bound=17.5\n",
      "3: loss=0.491, reward_mean=15.7, reward_bound=16.0\n",
      "4: loss=0.357, reward_mean=12.4, reward_bound=13.0\n",
      "5: loss=0.344, reward_mean=12.4, reward_bound=14.0\n",
      "6: loss=0.189, reward_mean=10.6, reward_bound=12.0\n",
      "7: loss=0.189, reward_mean=10.5, reward_bound=11.0\n",
      "8: loss=0.152, reward_mean=10.9, reward_bound=10.0\n",
      "9: loss=0.087, reward_mean=9.5, reward_bound=10.0\n",
      "10: loss=0.067, reward_mean=9.9, reward_bound=10.0\n",
      "11: loss=0.148, reward_mean=10.5, reward_bound=10.0\n",
      "12: loss=0.048, reward_mean=10.1, reward_bound=10.0\n",
      "13: loss=0.057, reward_mean=10.4, reward_bound=10.0\n",
      "14: loss=0.027, reward_mean=9.9, reward_bound=10.0\n",
      "15: loss=0.029, reward_mean=9.5, reward_bound=10.0\n",
      "16: loss=0.026, reward_mean=9.6, reward_bound=10.0\n",
      "17: loss=0.035, reward_mean=10.1, reward_bound=10.0\n",
      "18: loss=0.033, reward_mean=9.8, reward_bound=10.0\n",
      "19: loss=0.013, reward_mean=9.4, reward_bound=10.0\n",
      "20: loss=0.014, reward_mean=9.6, reward_bound=10.0\n",
      "21: loss=0.052, reward_mean=9.5, reward_bound=10.0\n",
      "22: loss=0.011, reward_mean=9.4, reward_bound=10.0\n",
      "23: loss=0.042, reward_mean=9.5, reward_bound=9.5\n",
      "24: loss=0.009, reward_mean=9.5, reward_bound=10.0\n",
      "25: loss=0.008, reward_mean=9.8, reward_bound=10.0\n",
      "26: loss=0.002, reward_mean=9.2, reward_bound=10.0\n",
      "27: loss=0.007, reward_mean=9.4, reward_bound=10.0\n",
      "28: loss=0.026, reward_mean=9.2, reward_bound=10.0\n",
      "29: loss=0.003, reward_mean=9.3, reward_bound=10.0\n",
      "30: loss=0.002, reward_mean=9.6, reward_bound=10.0\n",
      "31: loss=0.002, reward_mean=9.4, reward_bound=10.0\n",
      "32: loss=0.002, reward_mean=9.4, reward_bound=10.0\n",
      "33: loss=0.002, reward_mean=8.9, reward_bound=9.5\n",
      "34: loss=0.001, reward_mean=9.2, reward_bound=10.0\n",
      "35: loss=0.002, reward_mean=9.4, reward_bound=10.0\n",
      "36: loss=0.001, reward_mean=9.4, reward_bound=10.0\n",
      "37: loss=0.001, reward_mean=9.4, reward_bound=10.0\n",
      "38: loss=0.001, reward_mean=9.3, reward_bound=10.0\n",
      "39: loss=0.001, reward_mean=9.2, reward_bound=9.0\n",
      "40: loss=0.001, reward_mean=9.6, reward_bound=10.0\n",
      "41: loss=0.001, reward_mean=9.3, reward_bound=10.0\n",
      "42: loss=0.001, reward_mean=9.4, reward_bound=10.0\n",
      "43: loss=0.001, reward_mean=9.6, reward_bound=10.0\n",
      "44: loss=0.001, reward_mean=9.6, reward_bound=10.0\n",
      "45: loss=0.004, reward_mean=9.9, reward_bound=10.0\n",
      "46: loss=0.001, reward_mean=9.2, reward_bound=9.0\n",
      "47: loss=0.001, reward_mean=9.4, reward_bound=10.0\n",
      "48: loss=0.021, reward_mean=9.7, reward_bound=10.0\n",
      "49: loss=0.001, reward_mean=9.0, reward_bound=9.0\n",
      "50: loss=0.001, reward_mean=9.4, reward_bound=10.0\n",
      "51: loss=0.001, reward_mean=9.4, reward_bound=10.0\n",
      "52: loss=0.001, reward_mean=9.5, reward_bound=10.0\n",
      "53: loss=0.001, reward_mean=9.3, reward_bound=9.5\n",
      "54: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "55: loss=0.001, reward_mean=9.4, reward_bound=10.0\n",
      "56: loss=0.000, reward_mean=9.3, reward_bound=9.5\n",
      "57: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "58: loss=0.001, reward_mean=9.4, reward_bound=10.0\n",
      "59: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "60: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "61: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "62: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "63: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "64: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "65: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "66: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "67: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "68: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "69: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "70: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "71: loss=0.005, reward_mean=9.8, reward_bound=10.0\n",
      "72: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "73: loss=0.000, reward_mean=9.9, reward_bound=10.0\n",
      "74: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "75: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "76: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "77: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "78: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "79: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "80: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "81: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "82: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "83: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "84: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "85: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "86: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "87: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "88: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "89: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "90: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "91: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "92: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "93: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "94: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "95: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "96: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "97: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "98: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "99: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "100: loss=0.000, reward_mean=9.3, reward_bound=9.5\n",
      "101: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "102: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "103: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "104: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "105: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "106: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "107: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "108: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "109: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "110: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "111: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "112: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "113: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "114: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "115: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "116: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "117: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "118: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "119: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "120: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "121: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "122: loss=0.003, reward_mean=9.6, reward_bound=10.0\n",
      "123: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "124: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "125: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "126: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "127: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "128: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "129: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "130: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "131: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "132: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "133: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "134: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "135: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "136: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "137: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "138: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "139: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "140: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "141: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "142: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "143: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "144: loss=0.000, reward_mean=9.3, reward_bound=9.5\n",
      "145: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "146: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "147: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "148: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "149: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "150: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "151: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "152: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "153: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "154: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "155: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "156: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "157: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "158: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "159: loss=0.000, reward_mean=8.9, reward_bound=9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "161: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "162: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "163: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "164: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "165: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "166: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "167: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "168: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "169: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "170: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "171: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "172: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "173: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "174: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "175: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "176: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "177: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "178: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "179: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "180: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "181: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "182: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "183: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "184: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "185: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "186: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "187: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "188: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "189: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "190: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "191: loss=0.001, reward_mean=9.6, reward_bound=10.0\n",
      "192: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "193: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "194: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "195: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "196: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "197: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "198: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "199: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "200: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "201: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "202: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "203: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "204: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "205: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "206: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "207: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "208: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "209: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "210: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "211: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "212: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "213: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "214: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "215: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "216: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "217: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "218: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "219: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "220: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "221: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "222: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "223: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "224: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "225: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "226: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "227: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "228: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "229: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "230: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "231: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "232: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "233: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "234: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "235: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "236: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "237: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "238: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "239: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "240: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "241: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "242: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "243: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "244: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "245: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "246: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "247: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "248: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "249: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "250: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "251: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "252: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "253: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "254: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "255: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "256: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "257: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "258: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "259: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "260: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "261: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "262: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "263: loss=0.000, reward_mean=8.9, reward_bound=9.0\n",
      "264: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "265: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "266: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "267: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "268: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "269: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "270: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "271: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "272: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "273: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "274: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "275: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "276: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "277: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "278: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "279: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "280: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "281: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "282: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "283: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "284: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "285: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "286: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "287: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "288: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "289: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "290: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "291: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "292: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "293: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "294: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "295: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "296: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "297: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "298: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "299: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "300: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "301: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "302: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "303: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "304: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "305: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "306: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "307: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "308: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "309: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "310: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "311: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "312: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "313: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "314: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "315: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "316: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "317: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "318: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "319: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "320: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "321: loss=0.000, reward_mean=8.9, reward_bound=9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "323: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "324: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "325: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "326: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "327: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "328: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "329: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "330: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "331: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "332: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "333: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "334: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "335: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "336: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "337: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "338: loss=0.010, reward_mean=9.5, reward_bound=10.0\n",
      "339: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "340: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "341: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "342: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "343: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "344: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "345: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "346: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "347: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "348: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "349: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "350: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "351: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "352: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "353: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "354: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "355: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "356: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "357: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "358: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "359: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "360: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "361: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "362: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "363: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "364: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "365: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "366: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "367: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "368: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "369: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "370: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "371: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "372: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "373: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "374: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "375: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "376: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "377: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "378: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "379: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "380: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "381: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "382: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "383: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "384: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "385: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "386: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "387: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "388: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "389: loss=0.000, reward_mean=9.3, reward_bound=9.5\n",
      "390: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "391: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "392: loss=0.000, reward_mean=9.3, reward_bound=9.0\n",
      "393: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "394: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "395: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "396: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "397: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "398: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "399: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "400: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "401: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "402: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "403: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "404: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "405: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "406: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "407: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "408: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "409: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "410: loss=0.000, reward_mean=8.9, reward_bound=9.0\n",
      "411: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "412: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "413: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "414: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "415: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "416: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "417: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "418: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "419: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "420: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "421: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "422: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "423: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "424: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "425: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "426: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "427: loss=0.000, reward_mean=9.4, reward_bound=9.5\n",
      "428: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "429: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "430: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "431: loss=0.000, reward_mean=9.0, reward_bound=9.0\n",
      "432: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "433: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "434: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "435: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "436: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "437: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "438: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "439: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "440: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "441: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "442: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "443: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "444: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "445: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "446: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "447: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "448: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "449: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "450: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "451: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "452: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "453: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "454: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "455: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "456: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "457: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "458: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "459: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "460: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "461: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "462: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "463: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "464: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "465: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "466: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "467: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "468: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "469: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "470: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "471: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "472: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "473: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "474: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "475: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "476: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "477: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "478: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "479: loss=0.000, reward_mean=8.9, reward_bound=9.0\n",
      "480: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "481: loss=0.000, reward_mean=9.6, reward_bound=10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482: loss=0.000, reward_mean=9.3, reward_bound=9.5\n",
      "483: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "484: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "485: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "486: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "487: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "488: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "489: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "490: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "491: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "492: loss=0.000, reward_mean=9.0, reward_bound=9.0\n",
      "493: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "494: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "495: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "496: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "497: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "498: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "499: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "500: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "501: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "502: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "503: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "504: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "505: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "506: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "507: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "508: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "509: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "510: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "511: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "512: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "513: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "514: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "515: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "516: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "517: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "518: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "519: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "520: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "521: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "522: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "523: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "524: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "525: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "526: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "527: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "528: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "529: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "530: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "531: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "532: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "533: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "534: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "535: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "536: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "537: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "538: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "539: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "540: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "541: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "542: loss=0.021, reward_mean=9.2, reward_bound=9.5\n",
      "543: loss=0.000, reward_mean=9.9, reward_bound=10.0\n",
      "544: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "545: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "546: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "547: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "548: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "549: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "550: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "551: loss=0.000, reward_mean=9.9, reward_bound=10.0\n",
      "552: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "553: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "554: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "555: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "556: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "557: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "558: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "559: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "560: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "561: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "562: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "563: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "564: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "565: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "566: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "567: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "568: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "569: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "570: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "571: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "572: loss=0.000, reward_mean=9.0, reward_bound=9.0\n",
      "573: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "574: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "575: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "576: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "577: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "578: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "579: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "580: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "581: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "582: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "583: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "584: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "585: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "586: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "587: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "588: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "589: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "590: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "591: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "592: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "593: loss=0.000, reward_mean=9.4, reward_bound=9.5\n",
      "594: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "595: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "596: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "597: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "598: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "599: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "600: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "601: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "602: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "603: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "604: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "605: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "606: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "607: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "608: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "609: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "610: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "611: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "612: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "613: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "614: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "615: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "616: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "617: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "618: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "619: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "620: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "621: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "622: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "623: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "624: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "625: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "626: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "627: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "628: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "629: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "630: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "631: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "632: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "633: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "634: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "635: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "636: loss=0.000, reward_mean=8.9, reward_bound=9.0\n",
      "637: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "638: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "639: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "640: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "641: loss=0.000, reward_mean=9.5, reward_bound=10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "643: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "644: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "645: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "646: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "647: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "648: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "649: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "650: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "651: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "652: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "653: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "654: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "655: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "656: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "657: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "658: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "659: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "660: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "661: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "662: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "663: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "664: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "665: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "666: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "667: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "668: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "669: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "670: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "671: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "672: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "673: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "674: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "675: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "676: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "677: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "678: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "679: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "680: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "681: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "682: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "683: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "684: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "685: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "686: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "687: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "688: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "689: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "690: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "691: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "692: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "693: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "694: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "695: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "696: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "697: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "698: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "699: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "700: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "701: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "702: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "703: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "704: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "705: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "706: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "707: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "708: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "709: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "710: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "711: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "712: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "713: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "714: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "715: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "716: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "717: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "718: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "719: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "720: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "721: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "722: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "723: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "724: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "725: loss=0.000, reward_mean=9.3, reward_bound=9.5\n",
      "726: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "727: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "728: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "729: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "730: loss=0.000, reward_mean=8.9, reward_bound=10.0\n",
      "731: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "732: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "733: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "734: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "735: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "736: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "737: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "738: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "739: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "740: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "741: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "742: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "743: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "744: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "745: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "746: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "747: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "748: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "749: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "750: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "751: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "752: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "753: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "754: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "755: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "756: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "757: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "758: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "759: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "760: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "761: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "762: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "763: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "764: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "765: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "766: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "767: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "768: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "769: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "770: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "771: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "772: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "773: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "774: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "775: loss=0.000, reward_mean=8.9, reward_bound=9.0\n",
      "776: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "777: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "778: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "779: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "780: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "781: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "782: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "783: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "784: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "785: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "786: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "787: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "788: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "789: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "790: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "791: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "792: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "793: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "794: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "795: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "796: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "797: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "798: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "799: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "800: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "801: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "802: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "803: loss=0.000, reward_mean=9.2, reward_bound=10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "805: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "806: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "807: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "808: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "809: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "810: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "811: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "812: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "813: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "814: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "815: loss=0.000, reward_mean=9.3, reward_bound=9.5\n",
      "816: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "817: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "818: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "819: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "820: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "821: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "822: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "823: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "824: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "825: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "826: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "827: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "828: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "829: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "830: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "831: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "832: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "833: loss=0.000, reward_mean=8.8, reward_bound=9.0\n",
      "834: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "835: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "836: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "837: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "838: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "839: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "840: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "841: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "842: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "843: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "844: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "845: loss=0.000, reward_mean=9.0, reward_bound=9.0\n",
      "846: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "847: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "848: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "849: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "850: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "851: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "852: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "853: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "854: loss=0.000, reward_mean=9.0, reward_bound=9.5\n",
      "855: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "856: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "857: loss=0.000, reward_mean=9.3, reward_bound=9.5\n",
      "858: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "859: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "860: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "861: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "862: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "863: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "864: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "865: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "866: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "867: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "868: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "869: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "870: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "871: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "872: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "873: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "874: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "875: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "876: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "877: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "878: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "879: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "880: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "881: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "882: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "883: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "884: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "885: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "886: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "887: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "888: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "889: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "890: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "891: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "892: loss=0.000, reward_mean=9.0, reward_bound=9.0\n",
      "893: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "894: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "895: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "896: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "897: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "898: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "899: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "900: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "901: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "902: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "903: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "904: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "905: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "906: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "907: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "908: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "909: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "910: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "911: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "912: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "913: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "914: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "915: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "916: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "917: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "918: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "919: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "920: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "921: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "922: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "923: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "924: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "925: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "926: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "927: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "928: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "929: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "930: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "931: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "932: loss=0.000, reward_mean=9.2, reward_bound=9.0\n",
      "933: loss=0.000, reward_mean=9.8, reward_bound=10.0\n",
      "934: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "935: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "936: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "937: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "938: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "939: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "940: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "941: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "942: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "943: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "944: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "945: loss=0.000, reward_mean=9.1, reward_bound=10.0\n",
      "946: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "947: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "948: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "949: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "950: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "951: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "952: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "953: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "954: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "955: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "956: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "957: loss=0.000, reward_mean=9.1, reward_bound=9.5\n",
      "958: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "959: loss=0.000, reward_mean=9.6, reward_bound=10.0\n",
      "960: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "961: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "962: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "963: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "964: loss=0.000, reward_mean=9.2, reward_bound=10.0\n",
      "965: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "966: loss=0.000, reward_mean=9.6, reward_bound=10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967: loss=0.000, reward_mean=9.4, reward_bound=10.0\n",
      "968: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "969: loss=0.000, reward_mean=9.1, reward_bound=9.0\n",
      "970: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "971: loss=0.000, reward_mean=9.3, reward_bound=10.0\n",
      "972: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "973: loss=0.000, reward_mean=9.5, reward_bound=10.0\n",
      "974: loss=0.000, reward_mean=9.7, reward_bound=10.0\n",
      "975: loss=0.000, reward_mean=9.2, reward_bound=9.5\n",
      "976: loss=0.000, reward_mean=9.5, reward_bound=10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f445df984e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_scores_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mloss_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n\u001b[1;32m    129\u001b[0m             iter_no, loss_v.item(), reward_m, reward_b))\n",
      "\u001b[0;32m~/anaconda3/envs/datascience/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A naive algorithm :\n",
    "1.Play N episodes using a current model(say a random to start with)\n",
    "2.Collect total reward for all the episodes and decide on a reward boundary say 70th percentile\n",
    "3.Use these episodes to train the policy...aka neural network\n",
    "4.Repeat step 1 until satisfied\n",
    "\"\"\"\n",
    "#Note: There is some bug in the following code.Unlike traditional software engineering....in DL..code fails sailently\n",
    "    #..the network just don't converge...extra credits if figured out..\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym \n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "#define all the variables\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self,n_obs,n_hidden,n_action):\n",
    "        super(SimpleNet,self).__init__()\n",
    "        self.simplenet = nn.Sequential(\n",
    "            nn.Linear(n_obs,n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden,n_action)\n",
    "        )\n",
    "    def forward(self,input):\n",
    "        return self.simplenet(input)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    " \n",
    "sft_max = nn.Softmax(dim = 1)   \n",
    "\n",
    "def get_action(obs,net):\n",
    "    \"\"\"\n",
    "    Takes observations and the function approximator to return the action to be taken by the agent\n",
    "    \n",
    "    \"\"\"\n",
    "    obs_tensor = torch.FloatTensor([obs])\n",
    "    act_probs_tensor = sft_max(net(obs_tensor))\n",
    "    act_probs = act_probs_tensor.data.numpy()[0]\n",
    "    return np.random.choice(len(act_probs),p=act_probs)\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_batches(env,net,batch_size):\n",
    "    \"\"\"\n",
    "    iterates through the environment generating batches of `batch_size` which later get's fed into filter_percentile.\n",
    "    Input: environment -> env\n",
    "           Function Approximator -> net\n",
    "           batch size -> batch_size\n",
    "    Output:A batch of episodes of given batch_size\n",
    "    \"\"\"\n",
    "    \n",
    "    batch = [] # A collection of episodes\n",
    "    init_obs = env.reset() #resets' the environment and gives the initial observation\n",
    "    episode_reward = 0\n",
    "    episode_steps = []\n",
    "    while True:\n",
    "        action = get_action(init_obs,net)\n",
    "        next_obs,reward,is_done,_  = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(next_obs,action))\n",
    "        if is_done :\n",
    "            batch.append(Episode(episode_reward,episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                \n",
    "        init_obs = next_obs    \n",
    "        \n",
    "def filter_percentile(batch,percentile):\n",
    "    \"\"\"\n",
    "    Takes the batch and returns only top 70 percentile .\n",
    "    \n",
    "    \"\"\"\n",
    "    rewards = [element.reward for element in batch]\n",
    "    reward_bound = np.percentile(rewards,percentile)\n",
    "    reward_mean = float(np.mean(rewards)) #only for logging\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for ele in batch:\n",
    "        if ele.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend([steps.observation for steps in ele.steps])    \n",
    "        train_act.extend([steps.action for steps in ele.steps])\n",
    "    \n",
    "    train_obs_tensor = torch.FloatTensor(train_obs)\n",
    "    train_act_tensor = torch.LongTensor(train_act)\n",
    "    return train_obs_tensor, train_act_tensor, reward_bound, reward_mean\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env,directory='mon',force = True)\n",
    "n_obs = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "net = SimpleNet(n_obs,HIDDEN_SIZE,n_action)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params = net.parameters(),lr = 0.01)\n",
    "writer = SummaryWriter(comment = '-cartpole')    \n",
    "    \n",
    "    \n",
    "for iter_no,batch in enumerate(get_batches(env,net,BATCH_SIZE)):\n",
    "    obs_v,acts_v,reward_b,reward_m = filter_percentile(batch,PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "#     writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "#     writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "#     writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "    #writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tabular Q-learning :\n",
    "1.Act in the environment\n",
    "2.Get (s,a,r,s′) from the environment\n",
    "3.Update using bellmann-equation\n",
    "4.Check for convergance condition and if not repeat step-2\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "        return (old_state, action, reward, new_state)\n",
    "\n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_val = r + GAMMA * best_v\n",
    "        old_val = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_val * (1-ALPHA) + new_val * ALPHA\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.150\n",
      "Best reward updated 0.150 -> 0.200\n",
      "Best reward updated 0.200 -> 0.250\n",
      "Best reward updated 0.250 -> 0.300\n",
      "Best reward updated 0.300 -> 0.400\n",
      "Best reward updated 0.400 -> 0.450\n",
      "Best reward updated 0.450 -> 0.500\n",
      "Best reward updated 0.500 -> 0.550\n",
      "Best reward updated 0.550 -> 0.600\n",
      "Best reward updated 0.600 -> 0.650\n",
      "Best reward updated 0.650 -> 0.700\n",
      "Best reward updated 0.700 -> 0.750\n",
      "Best reward updated 0.750 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 6771 iterations!\n"
     ]
    }
   ],
   "source": [
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        s, a, r, next_s = agent.sample_env()\n",
    "        agent.value_update(s, a, r, next_s)\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep q learning:\n",
    "\n",
    "Reference : [Playing atari with deep reinforcemet learning - 2013 Minh](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deep Q Learning:\n",
    "\n",
    "1.Take initial action in the environment and collect (s,a,r,s^)\n",
    "\n",
    "2.Use any function approximator to estimate the quality of action.\n",
    "\n",
    "3.Update Q = r if the state is terminal else Q = r + gamma*max(Q(s^,a^)).\n",
    "\n",
    "4.Use sgd/it's variant for optimazation....repeat step 2 until convergance.\n",
    "\n",
    "# Note: Why the above naive implementation will not work ?(They will be resolved towards the end..)\n",
    "\n",
    "* One of the fundamental assumption of sgd is data is independent and identically distributed(i.i.d) which isn't the case here.\n",
    "* Distribution of training data is different from the one generated by optimal policy.\n",
    "* When using bellmann-equation to update the value of Q(s,a) using Q(s^,a^) ..we only have one step difference between them which makes both Q(s,a) and Q(s^,a^) highly correlated.Also when we are updating the weights ... this also changes our estimate of Q(s^,a^)...i.e we are chasing our tail.\n",
    "* Markov property(present state isn't the complete statistic of the future...) is one of the key assumption which behind all the classic RL algorithms including Deep Q learning but it often fails in practice.\n",
    "\n",
    "\n",
    "\n",
    "SO,what to do now ?\n",
    "\n",
    "* Many tricks have been developed over the years after the initial publication of DQN in 2013...here i will share the best of them and train a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wrappers are implemented here : https://github.com/openai/baselines/blob/master/baselines/common/atari_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: The following is an implementation of deep mind's orginal paper but optimized for this environment along with some changes to the model \n",
    "# architecture like adding `batch-norm`.Their orginal impl would take around a day to converge on gtx 1070 ..following hopefully will converge in `few hours`..due to some changes like ..\tin 100,000\tframes,\tepsilon\tis\tlinearly\tdecayed\tto\t 0.02 ,\twhich\tcorrespondsto\tthe\trandom\taction\ttaken\tin\t2%\tof\tsteps.\tA\tsimilar\tscheme\twas\tused\tin\tthe original\tDeepMind\tpaper,\tbut\tthe\tduration\tof\tdecay\twas\t10\ttimes\tlonger\t(so, epsilon\t=\t0.02\tis\treached\tafter\ta\tmillion\tframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN THE INTEREST OF TIME THIS IS NOT TRAINED...\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n",
    "        nn.BatchNorm2d(nf, momentum=0.01),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            conv_layer(input_shape[0],32,ks = 8,stride = 4),\n",
    "            conv_layer(32,64,ks = 4,stride = 2),\n",
    "            conv_layer(64,64,ks = 3,stride = 1)\n",
    "            \n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):  # A hack to flatten the layer without hard coding ..\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now similar to the above methods we can run any of the atari games on this network but before that let's proceed onto `DeepTrader`.....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above i have mentioned couple of reasons why naive implementation of DQN will not work..here i will use all the hacks in one go...\n",
    "\n",
    "REFER - >  Rainbow:Combining Improvements\tin\tDeep\tReinforcement\tLearning\n",
    "\n",
    "* ## N-stpes DQN : Unroll the bellmann equation\n",
    "* ## Double DQN : To deal with DQN overestimation of the values of actions\n",
    "* ## NOISY NETWORKS : Add noise to the weights to make exploration more efficient\n",
    "* Reference paper:Noisy\tNetworks for\tExploration\n",
    "* ## Prioritized replay buffer : Why uniform sampling from the buffer isn't the best option\n",
    "   * `NOTE`: The\tauthors\tof\tthe\tpaper\tquestioned\tthe\tuniform\trandom\tsample\tpolicy\tand proved\tthat\tby\tassigning\tpriorities\tto\tbuffer\tsamples,\taccording\tto\ttraining loss\tand\tsampling\tthe\tbuffer\tproportional\tto\tthose\tpriorities,\twe\tcan significantly\timprove\tconvergence\tand\tthe\tpolicy\tquality\tof\tthe\tDQN.\tThis method\tcan\tbe\tseen\tas\t“train\tmore\ton\tdata\tthat\tsurprises\tyou”.\tThe\ttricky point\there\tis\tto\tkeep\tthe\tbalance\tof\ttraining\ton\tan\t‘unusual’\tsample\tand training\ton\tthe\trest\tof\tthe\tbuffer.\tIf\twe\tfocus\tonly\ton\ta\tsmall\tsubset\tof\tthe buffer,\twe\tcan\tlose\tour\ti.i.d.\tproperty\tand\tsimply\toverfit\ton\tthis\tsubset.\n",
    "* ## Dueling DQN : How to improve convergance speed by making our network's architecture closer to problem we are solving\n",
    "    * `Refer` : Dueling Network\tArchitectures\tfor\tDeep\tReinforcement\tLearning\t([8]\tWang\tet\tal.,2015)\n",
    "* ## Categorical DQN : How to get distributions over Q values.(This also helps us got confidence bound on our predicted Q values which is very useful in financial setting...)\n",
    "    * `Refer`:Distributional\tPerspective\tOn\tReinforcement\tLearning.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "* `Complete list of references `:\n",
    "1.  Matteo\tHessel,\tJoseph\tModayil,\tHado\tvan\tHasselt,\tTom\tSchaul,\tGeorg\n",
    "Ostrovski,\tWill\tDabney,\tDan\tHorgan,\tBilal\tPiot,\tMohammad\tAzar,\n",
    "David\tSilver,\t2017,\tRainbow:\tCombining\tImprovements\tin\tDeep\n",
    "Reinforcement\tLearning.\tarXiv:1710.02298\n",
    "2.\t Sutton,\tR.S.\t1988,\tLearning\tto\tPredict\tby\tthe\tMethods\tof\tTemporal\n",
    "Differences,\tMachine\tLearning\t3(1):9-44\n",
    "3.\t Hado\tVan\tHasselt,\tArthur\tGuez,\tDavid\tSilver,\t2015,\tDeep\tReinforcement\n",
    "Learning\twith\tDouble\tQ-Learning.\tarXiv:1509.06461v3\n",
    "4.\t Meire\tFortunato,\tMohammad\tGheshlaghi\tAzar,\tBilal\tPilot,\tJacob\n",
    "Menick,\tIan\tOsband,\tAlex\tGraves,\tVlad\tMnih,\tRemi\tMunos,\tDemis\n",
    "Hassabis,\tOlivier\tPietquin,\tCharles\tBlundell,\tShane\tLegg,\t2017,\tNoisy\n",
    "Networks\tfor\tExploration\tarXiv:1706.10295v1\n",
    "5.\t Marc\tBellemare,\tSriram\tSrinivasan,\tGeorg\tOstrovski,\tTom\tSchaus,\n",
    "David\tSaxton,\tRemi\tMunos\t2016,\tUnifying\tCount-Based\tExploration\n",
    "and\tIntrinsic\tMotivation\tarXiv:1606.01868v2\n",
    "6.\t Jarryd\tMartin,\tSuraj\tNarayanan\tSasikumar,\tTom\tEveritt,\tMarcus\tHutter,\n",
    "2017,\tCount-Based\tExploration\tin\tFeature\tSpace\tfor\tReinforcement\n",
    "Learning\tarXiv:1706.08090\n",
    "7.\t Tom\tSchaul,\tJohn\tQuan,\tIoannis\tAntonoglou,\tDavid\tSilver,\t2015,\n",
    "Prioritized\tExperience\tReplay\tarXiv:1511.05952\n",
    "8.\t Ziyu\tWang,\tTom\tSchaul,\tMatteo\tHessel,\tHado\tvan\tHasselt,\tMarc\n",
    "Lanctot,\tNando\tde\tFreitas,\t2015,\tDueling\tNetwork\tArchitectures\tfor\n",
    "Deep\tReinforcement\tLearning\tarXiv:1511.06581\n",
    "9.\t Marc\tG.\tBellemare,\tWill\tDabney,\tRémi\tMunos,\t2017,\tA\tDistributional\n",
    "Perspective\ton\tReinforcement\tLearning\tarXiv:1707.06887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Till now we only scratched the surface of Deep Reinforcement Learning.We can have agents that predict probability distribution over actions(policy gradient methods).Some times we need some kind of memory....for example single observation is not enough to make the decision...so might include past `k` observations into the `state`.There is a whole branch of RL dealing with POMDP(PARTIALLY OBSERVABLE DECISION PROCESS).To capture all the variants `DeepTrader` is implemented as heirarchy of classes inspired from `open ai gym` and `fastai`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LET'S BUILD A BASELINE FOR OUR DEEP TRADER AGENT :\n",
    "NOTE : 1. In our state we assume instant order execution at the last bar's `close` price.No slippage.\n",
    "       2. The episode ends once the agent decides to close the position.\n",
    "       3.0.1% commission is `assumed` for every executed trade.\n",
    "OBSERVATION WILL INCLUDE :\n",
    "    * N past (open,high,low,close,profit/loss from current position,indication of past holding)\n",
    "Action space :\n",
    "    * (DO NOTHING,BUY A SHARE,CLOSE THE POSITION.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer: train_model_conv.py\n",
    "#The core library / collection of abstractions to make a trading environment compatible with open ai gym...DeepTrader\n",
    "#is not included here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's some training in action ---- screenshots..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'niveshi.png'>\n",
    "<img src = 'niveshi_tensorflow.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
